# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from django import forms
from django.contrib import admin, messages
from thoth_core.models import (
    SqlDb,
    VectorDb,
    SqlTable,
    SqlColumn,
    Relationship,
    Workspace,
)
from thoth_core.utilities.utils import (
    export_csv,
    import_csv,
    export_db_structure_to_csv,
)
from thoth_core.dbmanagement import (
    create_tables,
    create_relationships,
    create_db_elements,
)
from thoth_core.thoth_ai.thoth_workflow.create_db_scope import generate_scope
from thoth_core.thoth_ai.thoth_workflow.generate_db_documentation import (
    generate_db_documentation,
)
from thoth_core.thoth_ai.thoth_workflow.gdpr_scanner import scan_database_for_gdpr
from thoth_core.thoth_ai.thoth_workflow.async_table_comments import (
    start_async_column_comments,
    start_async_table_comments,
)
from thoth_core.utilities.task_validation import check_task_can_start
from thoth_core.widgets import PasswordInputWithToggle


class SqlDbAdminForm(forms.ModelForm):
    class Meta:
        model = SqlDb
        fields = "__all__"
        help_texts = {
            "name": "Unique identifier for this database connection in Thoth (REQUIRED)",
            "db_host": 'Database host address. For Docker deployments, use the container name (e.g., "thoth-db" for PostgreSQL, "host.docker.internal" for local databases)',
            "db_type": "Type of SQL database to connect to (REQUIRED)",
            "db_name": "The actual database name on the server (REQUIRED)",
            "db_port": "Database port number (leave empty for default: PostgreSQL=5432, MySQL=3306, etc.)",
            "schema": "Database schema to use (leave empty for default schema)",
            "user_name": "Database username for authentication",
            "password": "Database password for authentication",
            "db_mode": "Environment mode (dev/test/prod) - affects connection behavior (REQUIRED)",
            "vector_db": "Associated vector database for storing semantic information about this SQL database",
            "language": "Primary language of the database content (used for AI processing)",
            "scope": "Description of the database purpose and content (generated or manually entered)",
            "scope_json": "JSON format of the database scope for programmatic use",
            "directives": "Free-form directives/hints for agents interacting with this DB",
            "erd": "Entity Relationship Diagram in Mermaid format (generated by AI documentation process)",
        }
        widgets = {
            "user_name": forms.TextInput(
                attrs={"style": "width: 500px; max-width: 100%;"}
            ),
            "password": PasswordInputWithToggle(render_value=True),
            "scope": forms.Textarea(attrs={"rows": 8, "cols": 160}),
            "scope_json": forms.Textarea(attrs={"rows": 8, "cols": 160}),
            "directives": forms.Textarea(attrs={"rows": 8, "cols": 160}),
            "erd": forms.Textarea(attrs={"rows": 12, "cols": 160}),
        }


@admin.register(SqlDb)
class SqlDbAdmin(admin.ModelAdmin):
    form = SqlDbAdminForm
    list_display = ("name", "db_host", "db_type", "db_name", "schema", "vector_db_name")
    search_fields = ("name", "db_host", "db_type", "db_name", "schema")
    actions = (
        export_csv,
        import_csv,
        create_tables,
        create_relationships,
        create_db_elements,
        "validate_db_fk_fields",
        export_db_structure_to_csv,
        "duplicate_sqldb",
        "associate_candidates_fk_to_pk",
        "test_connection",
        "generate_all_comments",
        generate_scope,
        generate_db_documentation,
        "scan_gdpr_compliance",
    )
    fieldsets = (
        (
            "Basic Information",
            {
                "fields": ("name", "db_type", "db_mode"),
                "description": "All fields in this section are required.",
            },
        ),
        (
            "Connection Details",
            {
                "fields": ("db_host", "db_port", "db_name", "schema"),
                "description": 'Docker Host Configuration: For databases in Docker containers, use container name (e.g., "thoth-db"). For local databases accessed from Docker, use "host.docker.internal". For external databases, use the actual hostname or IP address. Note: db_name field is required.',
            },
        ),
        (
            "Authentication",
            {
                "fields": ("user_name", "password"),
                "classes": ("collapse",),
            },
        ),
        (
            "AI Configuration",
            {
                "fields": (
                    "vector_db",
                    "language",
                    "scope",
                    "scope_json",
                    "directives",
                    "erd",
                ),
                "classes": ("collapse",),
            },
        ),
        (
            "GDPR Compliance",
            {
                "fields": ("gdpr_report", "gdpr_scan_date"),
                "classes": ("collapse",),
                "description": "GDPR compliance scan results and metadata",
            },
        ),
        (
            "Metadata",
            {
                "fields": ("last_columns_update",),
                "classes": ("collapse",),
                "description": "System-managed metadata",
            },
        ),
    )

    def vector_db_name(self, obj):
        return obj.vector_db.name if obj.vector_db else None

    vector_db_name.short_description = "Vector DB"

    def formfield_for_foreignkey(self, db_field, request, **kwargs):
        if db_field.name == "vector_db":
            kwargs["queryset"] = VectorDb.objects.all().order_by("name")
        return super().formfield_for_foreignkey(db_field, request, **kwargs)

    def duplicate_sqldb(self, request, queryset):
        """
        Duplicate selected SqlDb instances with name + " copy"
        Also duplicates associated VectorDb with "_copy" suffix
        """
        duplicated_count = 0

        for sqldb in queryset:
            try:
                # Store the original name for error messages
                original_name = sqldb.name
                original_vector_db = sqldb.vector_db

                # Duplicate the associated VectorDb first if it exists
                duplicated_vector_db = None
                if original_vector_db:
                    # Create a copy of the VectorDb
                    duplicated_vector_db = VectorDb.objects.get(
                        pk=original_vector_db.pk
                    )
                    duplicated_vector_db.pk = None
                    duplicated_vector_db.id = None
                    duplicated_vector_db.name = f"{original_vector_db.name}_copy"
                    duplicated_vector_db.save()

                # Create the duplicate SqlDb
                sqldb.pk = None  # This will create a new instance when saved
                sqldb.id = None  # Ensure the ID is also reset
                sqldb.name = f"{original_name} copy"

                # Link to the duplicated VectorDb
                sqldb.vector_db = duplicated_vector_db

                # Reset timestamp fields to None for the copy
                sqldb.last_columns_update = None

                sqldb.save()
                duplicated_count += 1

            except Exception as e:
                messages.error(
                    request, f"Error duplicating SqlDb '{original_name}': {str(e)}"
                )
                continue

        if duplicated_count > 0:
            messages.success(
                request,
                f"Successfully duplicated {duplicated_count} SqlDb(s) with their associated Vector databases.",
            )
        else:
            messages.warning(request, "No SqlDb instances were duplicated.")

    duplicate_sqldb.short_description = (
        "Duplicate selected SqlDb instances with VectorDb"
    )

    def associate_candidates_fk_to_pk(self, request, queryset):
        """
        Automatically identifies and creates foreign key relationships by analyzing column naming patterns.

        Detects relationships using 6 naming patterns:
        1. Exact match: id → users.id
        2. Snake case: user_id → users.id
        3. Kebab case: user-id → users.id
        4. Camel case: userId → users.id
        5. Simple concatenation: userid → users.id
        6. Table name only: user → users.id

        Validates relationships by checking that 70% of FK values exist in the referenced table.
        """
        from thoth_core.dbmanagement import get_db_manager

        total_databases = 0
        total_fks_identified = 0
        total_fks_validated = 0
        total_relationships_created = 0

        for sql_db in queryset:
            total_databases += 1

            try:
                # Get database manager
                db_manager = get_db_manager(sql_db)
                if not db_manager:
                    messages.error(
                        request,
                        f"Database '{sql_db.name}': Could not create database manager",
                    )
                    continue

                # Step 1: Find all primary key columns
                pk_columns = []
                all_columns = SqlColumn.objects.filter(sql_table__sql_db=sql_db)

                for column in all_columns:
                    # Check if this is a primary key based on various indicators
                    is_pk = (
                        column.pk_field
                        and "PK" in column.pk_field.upper()
                        or column.original_column_name.lower() in ["id", "pk"]
                        or column.original_column_name.lower().endswith("_id")
                        and column.pk_field
                    )

                    if is_pk:
                        pk_columns.append(column)

                messages.info(
                    request,
                    f"Database '{sql_db.name}': Found {len(pk_columns)} primary key columns across {SqlTable.objects.filter(sql_db=sql_db).count()} tables",
                )

                # Step 2: Find potential foreign key relationships
                potential_relationships = []

                for fk_column in all_columns:
                    if fk_column in pk_columns:
                        continue  # Skip primary key columns

                    fk_table_name = fk_column.sql_table.name.lower()
                    fk_column_name = fk_column.original_column_name.lower()

                    for pk_column in pk_columns:
                        pk_table_name = pk_column.sql_table.name.lower()
                        pk_column_name = pk_column.original_column_name.lower()

                        # Skip self-references to avoid infinite loops
                        if fk_table_name == pk_table_name:
                            continue

                        # Pattern 1: Exact match
                        if fk_column_name == pk_column_name:
                            potential_relationships.append(
                                (fk_column, pk_column, "exact_match")
                            )
                            continue

                        # Pattern 2: Snake case (tablename_pkname)
                        expected_snake = f"{pk_table_name}_{pk_column_name}"
                        if fk_column_name == expected_snake:
                            potential_relationships.append(
                                (fk_column, pk_column, "snake_case")
                            )
                            continue

                        # Pattern 3: Kebab case (tablename-pkname)
                        expected_kebab = f"{pk_table_name}-{pk_column_name}"
                        if fk_column_name == expected_kebab:
                            potential_relationships.append(
                                (fk_column, pk_column, "kebab_case")
                            )
                            continue

                        # Pattern 4: Camel case (tablenamePkname)
                        expected_camel = f"{pk_table_name}{pk_column_name.capitalize()}"
                        if fk_column_name == expected_camel:
                            potential_relationships.append(
                                (fk_column, pk_column, "camel_case")
                            )
                            continue

                        # Pattern 5: Simple concatenation (tablenamepkname)
                        expected_concat = f"{pk_table_name}{pk_column_name}"
                        if fk_column_name == expected_concat:
                            potential_relationships.append(
                                (fk_column, pk_column, "concatenation")
                            )
                            continue

                        # Pattern 6: Table name only (tablename)
                        if fk_column_name == pk_table_name:
                            potential_relationships.append(
                                (fk_column, pk_column, "table_name_only")
                            )
                            continue

                total_fks_identified += len(potential_relationships)
                messages.info(
                    request,
                    f"Database '{sql_db.name}': Identified {len(potential_relationships)} potential FK relationships",
                )

                # Step 3: Validate relationships by checking data integrity
                validated_relationships = []

                for fk_column, pk_column, pattern in potential_relationships:
                    try:
                        # Get sample of FK values (non-null)
                        fk_table_name = fk_column.sql_table.name
                        fk_column_name = fk_column.original_column_name
                        pk_table_name = pk_column.sql_table.name
                        pk_column_name = pk_column.original_column_name

                        # Get 20 random non-null FK values
                        sample_query = f"""
                        SELECT DISTINCT {fk_column_name} 
                        FROM {fk_table_name} 
                        WHERE {fk_column_name} IS NOT NULL 
                        LIMIT 20
                        """

                        fk_values_result = db_manager.execute_sql(sample_query)
                        if not fk_values_result or len(fk_values_result) == 0:
                            continue

                        fk_values = [row[0] for row in fk_values_result]
                        if len(fk_values) == 0:
                            continue

                        # Validate each FK value exists in PK table
                        valid_count = 0
                        total_count = len(fk_values)

                        for fk_value in fk_values:
                            try:
                                # Try multiple parameter formats for compatibility
                                pk_query = f"SELECT COUNT(*) FROM {pk_table_name} WHERE {pk_column_name} = %s"

                                try:
                                    pk_results = db_manager.execute_sql(
                                        pk_query, (fk_value,)
                                    )
                                except Exception:
                                    try:
                                        # Fallback to ? parameter
                                        pk_query = f"SELECT COUNT(*) FROM {pk_table_name} WHERE {pk_column_name} = ?"
                                        pk_results = db_manager.execute_sql(
                                            pk_query, (fk_value,)
                                        )
                                    except Exception:
                                        # Last fallback - direct formatting (be careful with SQL injection)
                                        if isinstance(fk_value, str):
                                            pk_query = f"SELECT COUNT(*) FROM {pk_table_name} WHERE {pk_column_name} = '{fk_value}'"
                                        else:
                                            pk_query = f"SELECT COUNT(*) FROM {pk_table_name} WHERE {pk_column_name} = {fk_value}"
                                        pk_results = db_manager.execute_sql(pk_query)

                                if (
                                    pk_results
                                    and len(pk_results) > 0
                                    and pk_results[0][0] > 0
                                ):
                                    valid_count += 1

                            except Exception:
                                # Skip individual value validation errors
                                continue

                        # Require 70% validation rate
                        validation_rate = (
                            valid_count / total_count if total_count > 0 else 0
                        )
                        if validation_rate >= 0.7:
                            validated_relationships.append(
                                (fk_column, pk_column, pattern, validation_rate)
                            )

                    except Exception:
                        # Skip relationship validation errors and continue
                        continue

                total_fks_validated += len(validated_relationships)
                messages.info(
                    request,
                    f"Database '{sql_db.name}': Validated {len(validated_relationships)} FK relationships",
                )

                # Step 4: Create Relationship records
                relationships_created = 0

                for (
                    fk_column,
                    pk_column,
                    pattern,
                    validation_rate,
                ) in validated_relationships:
                    try:
                        relationship, created = Relationship.objects.get_or_create(
                            source_table=fk_column.sql_table,
                            target_table=pk_column.sql_table,
                            source_column=fk_column,
                            target_column=pk_column,
                        )

                        # Update FK field information
                        relationship.update_pk_fk_fields()
                        relationships_created += 1

                    except Exception:
                        # Continue processing other relationships despite individual errors
                        continue

                total_relationships_created += relationships_created
                messages.info(
                    request,
                    f"Database '{sql_db.name}': Processed {relationships_created} relationships (some may have overridden existing records)",
                )

            except Exception as e:
                messages.error(
                    request,
                    f"Database '{sql_db.name}': Error during FK association - {str(e)}",
                )
                continue

        # Final summary message
        messages.success(
            request,
            f"FK Association completed: {total_databases} databases processed, "
            f"{total_fks_identified} FKs identified, {total_fks_validated} validated, "
            f"{total_relationships_created} relationships created.",
        )

    associate_candidates_fk_to_pk.short_description = "Associate candidates FK to PK"

    def test_connection(self, request, queryset):
        """
        Test database connection for selected databases.
        """
        from thoth_core.dbmanagement import get_db_manager

        success_count = 0
        error_count = 0

        for sqldb in queryset:
            try:
                # Try to get database manager and connect
                db_manager = get_db_manager(sqldb)

                # Try a simple query to verify connection
                # Different databases might have different system tables
                test_queries = {
                    "postgresql": "SELECT 1",
                    "mysql": "SELECT 1",
                    "mariadb": "SELECT 1",
                    "sqlite": "SELECT 1",
                    "oracle": "SELECT 1 FROM DUAL",
                    "sqlserver": "SELECT 1",
                    "informix": "SELECT 1 FROM systables WHERE tabid = 1",
                    "supabase": "SELECT 1",
                }

                test_query = test_queries.get(sqldb.db_type.lower(), "SELECT 1")

                # Execute test query
                result = db_manager.execute_sql(test_query)

                if result is not None:
                    success_count += 1
                    messages.success(
                        request,
                        f"Database '{sqldb.name}' ({sqldb.db_type}) - Connection successful",
                    )
                else:
                    error_count += 1
                    messages.error(
                        request,
                        f"Database '{sqldb.name}' ({sqldb.db_type}) - Query returned no results",
                    )

            except Exception as e:
                error_count += 1
                error_msg = str(e)

                # Provide more specific error messages
                if "could not connect to server" in error_msg.lower():
                    messages.error(
                        request,
                        f"Database '{sqldb.name}' - Cannot connect to host '{sqldb.db_host}:{sqldb.db_port or 'default'}'",
                    )
                elif (
                    "authentication failed" in error_msg.lower()
                    or "password" in error_msg.lower()
                ):
                    messages.error(
                        request,
                        f"Database '{sqldb.name}' - Authentication failed for user '{sqldb.user_name}'",
                    )
                elif (
                    "database" in error_msg.lower()
                    and "does not exist" in error_msg.lower()
                ):
                    messages.error(
                        request,
                        f"Database '{sqldb.name}' - Database '{sqldb.db_name}' does not exist",
                    )
                else:
                    messages.error(
                        request,
                        f"Database '{sqldb.name}' - Connection failed: {error_msg}",
                    )

        # Summary message
        if success_count > 0 and error_count == 0:
            messages.success(
                request,
                f"All {success_count} database connection(s) tested successfully",
            )
        elif success_count > 0 and error_count > 0:
            messages.warning(
                request,
                f"Testing completed: {success_count} successful, {error_count} failed",
            )
        elif error_count > 0 and success_count == 0:
            messages.error(request, f"All {error_count} database connection(s) failed")

    test_connection.short_description = "Test database connection"

    def validate_db_fk_fields(self, request, queryset):
        """
        Validates the format of fk_field for all columns in all tables of the selected databases.
        """
        from thoth_core.admin import validate_fk_fields

        total_error_count = 0
        total_success_count = 0
        all_error_messages = []
        total_tables = 0
        total_checked = 0

        for db in queryset:
            tables = SqlTable.objects.filter(sql_db=db)
            total_tables += tables.count()

            for table in tables:
                columns = SqlColumn.objects.filter(sql_table=table)
                error_count, success_count, error_messages, checked = (
                    validate_fk_fields(columns, request)
                )

                total_error_count += error_count
                total_success_count += success_count
                all_error_messages.extend(error_messages)
                total_checked += checked

        max_errors_to_show = 50
        for error_msg in all_error_messages[:max_errors_to_show]:
            messages.error(request, error_msg)

        if len(all_error_messages) > max_errors_to_show:
            messages.warning(
                request,
                f"Showing only {max_errors_to_show} of {len(all_error_messages)} errors.",
            )

        # Display summary
        if total_error_count == 0:
            if total_checked > 0:
                messages.success(
                    request,
                    f"All {total_checked} foreign key references across {total_tables} tables in {queryset.count()} databases are valid.",
                )
            else:
                messages.info(
                    request,
                    f"No foreign key references found to validate in {total_tables} tables across {queryset.count()} databases.",
                )
        else:
            messages.warning(
                request,
                f"Validation completed with {total_error_count} errors and {total_success_count} valid references out of {total_checked} checked across {total_tables} tables in {queryset.count()} databases.",
            )

    validate_db_fk_fields.short_description = "Validate FK field format"

    def generate_all_comments(self, request, queryset):
        """
        Generate comments for all tables and columns in the selected database.
        First generates comments for all columns, then for all tables.
        Accepts only one database at a time and logs progress in workspace monitoring fields.
        """
        if queryset.count() != 1:
            messages.error(
                request, "Please select exactly one database for comment generation."
            )
            return

        sql_db = queryset.first()

        # Check if we have a current workspace
        if not hasattr(request, "current_workspace") or not request.current_workspace:
            messages.error(
                request, "No active workspace found. Please select a workspace."
            )
            return

        workspace = request.current_workspace

        # Verify that the selected database belongs to the current workspace
        if sql_db != workspace.sql_db:
            messages.error(
                request,
                f"Selected database '{sql_db.name}' does not belong to the current workspace database '{workspace.sql_db.name}'. "
                f"Please ensure you're working with the correct database.",
            )
            return

        # Check if column comment generation task can be started
        can_start_columns, column_message = check_task_can_start(
            workspace, "column_comment"
        )
        if not can_start_columns:
            messages.error(
                request,
                f"Cannot start column comment generation: {column_message}. Current column comment status: {workspace.column_comment_status}",
            )
            return

        # Check if table comment generation task can be started
        can_start_tables, table_message = check_task_can_start(
            workspace, "table_comment"
        )
        if not can_start_tables:
            messages.error(
                request,
                f"Cannot start table comment generation: {table_message}. Current table comment status: {workspace.table_comment_status}",
            )
            return

        # Get all columns and tables for this database
        all_columns = SqlColumn.objects.filter(sql_table__sql_db=sql_db)
        all_tables = SqlTable.objects.filter(sql_db=sql_db)

        if not all_columns.exists():
            messages.warning(
                request,
                f"No columns found in database '{sql_db.name}'. Please ensure the database structure has been imported first.",
            )
            return

        if not all_tables.exists():
            messages.warning(
                request,
                f"No tables found in database '{sql_db.name}'. Please ensure the database structure has been imported first.",
            )
            return

        # Get column and table IDs
        column_ids = list(all_columns.values_list("id", flat=True))
        table_ids = list(all_tables.values_list("id", flat=True))

        try:
            # Start with column comment generation first
            # The table comment generation will be triggered after columns complete
            from threading import Thread
            import time

            def sequential_comment_generation():
                """Execute comment generation sequentially: columns first, then tables"""
                try:
                    # Step 1: Generate column comments
                    column_task_id = start_async_column_comments(
                        workspace.id, column_ids, request.user.id
                    )

                    # Wait for column comments to complete before starting table comments
                    workspace.refresh_from_db()
                    while (
                        workspace.column_comment_status
                        == workspace.PreprocessingStatus.RUNNING
                    ):
                        time.sleep(5)  # Check every 5 seconds
                        workspace.refresh_from_db()

                    # Step 2: If columns completed successfully, start table comments
                    if (
                        workspace.column_comment_status
                        == workspace.PreprocessingStatus.COMPLETED
                    ):
                        table_task_id = start_async_table_comments(
                            workspace.id, table_ids, request.user.id
                        )
                    else:
                        # Log that table comments were skipped due to column comment failure
                        workspace.table_comment_log = "Table comment generation skipped due to column comment failure"
                        workspace.save()

                except Exception as e:
                    workspace.refresh_from_db()
                    workspace.table_comment_log = (
                        f"Sequential comment generation error: {str(e)}"
                    )
                    workspace.save()

            # Start the sequential process in a background thread
            thread = Thread(target=sequential_comment_generation)
            thread.daemon = True
            thread.start()

            # Initialize column comment status
            workspace.column_comment_status = Workspace.PreprocessingStatus.RUNNING
            workspace.column_comment_task_id = (
                f"sequential_comments_{workspace.id}_{int(time.time())}"
            )
            workspace.column_comment_log = f"Starting sequential comment generation: {len(column_ids)} columns, then {len(table_ids)} tables"

            # Initialize table comment status as pending
            workspace.table_comment_status = Workspace.PreprocessingStatus.IDLE
            workspace.table_comment_task_id = None
            workspace.table_comment_log = f"Waiting for column comments to complete before processing {len(table_ids)} tables"
            workspace.save()

            messages.success(
                request,
                f"Started sequential comment generation for database '{sql_db.name}' in workspace '{workspace.name}'. "
                f"Processing {len(column_ids)} columns first, then {len(table_ids)} tables. "
                f"Monitor progress in the workspace status fields.",
            )

        except Exception as e:
            messages.error(request, f"Error starting comment generation: {str(e)}")

    generate_all_comments.short_description = (
        "Generate comments for all tables and columns (AI assisted)"
    )

    def scan_gdpr_compliance(self, request, queryset):
        """
        Scan selected databases for GDPR-sensitive data and generate compliance reports.
        """
        try:
            # Check workspace
            if (
                not hasattr(request, "current_workspace")
                or not request.current_workspace
            ):
                self.message_user(
                    request,
                    "No active workspace found. Please select a workspace.",
                    messages.ERROR,
                )
                return

            # Process only the first database
            if queryset.count() > 1:
                self.message_user(
                    request,
                    "Multiple databases selected. Processing only the first one.",
                    messages.WARNING,
                )

            db = queryset.first()
            if not db:
                self.message_user(request, "No database selected.", messages.ERROR)
                return

            # Perform GDPR scan
            self.message_user(
                request,
                f"Starting GDPR compliance scan for database '{db.name}'...",
                messages.INFO,
            )

            report = scan_database_for_gdpr(db.id)

            if "error" in report:
                self.message_user(
                    request,
                    f"Error during GDPR scan: {report['error']}",
                    messages.ERROR,
                )
                return

            # Save report to database
            from django.utils import timezone

            db.gdpr_report = report
            db.gdpr_scan_date = timezone.now()
            db.save(update_fields=["gdpr_report", "gdpr_scan_date"])

            # Generate success message with summary
            summary = report["summary"]
            risk_score = report.get("risk_score", {})

            message = (
                f"Successfully completed GDPR compliance scan for database '{db.name}'. "
                f"Found {summary['sensitive_columns']} sensitive columns across {summary['tables_with_sensitive_data']} tables. "
                f"Risk Level: {risk_score.get('level', 'N/A')} ({risk_score.get('score', 0)}/100). "
            )

            if summary["critical_findings"] > 0:
                message += f"[CRITICAL] {summary['critical_findings']} critical findings require immediate attention. "

            self.message_user(
                request,
                message,
                messages.SUCCESS
                if summary["critical_findings"] == 0
                else messages.WARNING,
            )

        except Exception as e:
            self.message_user(
                request,
                f"An unexpected error occurred during GDPR scan: {str(e)}",
                messages.ERROR,
            )

    scan_gdpr_compliance.short_description = "Scan for GDPR-sensitive data"
